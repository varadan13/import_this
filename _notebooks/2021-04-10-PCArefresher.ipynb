{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-04-10-PCArefresher.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfNFDzeUM42U"
      },
      "source": [
        "# \"Please have a PCA refresher\"\n",
        "\n",
        "- author: \"<a href='https://www.linkedin.com/in/aneeshdata/'>Aneesh R</a>\"\n",
        "- toc: false\n",
        "- comments: true\n",
        "- categories: [Linear Algebra Tutorial]\n",
        "- badges: false"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg72Ko9YD9Yp"
      },
      "source": [
        "### Section 0\n",
        "\n",
        "> Codes and fetching data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59ODi1-Tq4YV"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm \n",
        "import tabulate as tb # to draw tables\n",
        "\n",
        "\n",
        "class udregression(object):\n",
        "  \"\"\"udregression stands for a class that is user defined to do regression\"\"\"\n",
        "  def __init__(self,X,Y,intercept=True):\n",
        "    self.X=X\n",
        "    self.Y=Y\n",
        "    self.intercept=intercept\n",
        "  \n",
        "  def fit_model(self):\n",
        "    if self.intercept==True:\n",
        "      X1=np.column_stack([[1 for _ in range(self.X.shape[0])],self.X])\n",
        "      model=sm.OLS(self.Y,X1)\n",
        "      f_model=model.fit()\n",
        "    else:\n",
        "      model=sm.OLS(self.Y,self.X)\n",
        "      f_model=model.fit() \n",
        "    return f_model\n",
        "  \n",
        "  def print_result(self):\n",
        "    fitted_model=self.fit_model()\n",
        "    length=len(fitted_model.summary().tables[1].data)\n",
        "    var=[i[0] for i in fitted_model.summary().tables[1].data[1:length]]\n",
        "    prtmp1={'var':var,'coef':fitted_model.params,\n",
        "          'st error':fitted_model.bse,'t values':fitted_model.tvalues,\n",
        "          'p values':fitted_model.pvalues,'LL':fitted_model.conf_int()[:,0],\n",
        "          'RL':fitted_model.conf_int()[:,1]}\n",
        "    \n",
        "    prtmp2={'rsquared':[fitted_model.rsquared],\n",
        "          'rsquared_Adj':[fitted_model.rsquared_adj],\n",
        "          'fvalue':[fitted_model.fvalue]}\n",
        "    \n",
        "    print(tb.tabulate(prtmp1,headers=\"keys\"))\n",
        "    print()\n",
        "    print(tb.tabulate(prtmp2,headers=\"keys\"))\n",
        "\n",
        "  def __repr__(self): return \"a custom regression runner has been intialised\"\n",
        "\n",
        "df=pd.read_csv('https://raw.githubusercontent.com/varadan13/Data_Bank/main/Econ%20data/MULTICOLLINEARITYPROB.csv')    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFUK6lD3-T6a"
      },
      "source": [
        "### Section 1\n",
        "\n",
        "Suppose we have a data of the form $(y,x_1,x_2)$. [for now let's think about them as points in a 3D space]\n",
        "\n",
        "let's assume $x_1$ and $x_2$ are linearly related.   \n",
        "\n",
        "Our current problem can be roughly summarised as follows:   \n",
        "predict y **<-** (by using information in $x_1$) & (by using information in $x_2$)  \n",
        "\n",
        "(1.1) But since we assumed that $x_1$ and $x_2$ are linearly related there may not be much information remaining in $x_2$ once we regress $x_1$ on y or there may not be much information remaining in $x_1$ once we regress $x_2$ on y. \n",
        "\n",
        "let's take an example to understand this.\n",
        "\n",
        "Below, we have stored a data set in 'data'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGFAIpZpAanM"
      },
      "source": [
        "data=df[['Hours','NEIN','Assets']] # NEIN and Assets are independent variable\n",
        "                                   # Hours is dependent variable\n",
        "                                   # So we are regressing NEIN and Assets on Hours\n",
        "\n",
        "# Let's check the correlation between NEIN and Assets "
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "QBHWeQONAdnE",
        "outputId": "360fc688-0f46-4589-fa87-fed8aad03202"
      },
      "source": [
        "data[['NEIN','Assets']].corr()"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NEIN</th>\n",
              "      <th>Assets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>NEIN</th>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.98751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Assets</th>\n",
              "      <td>0.98751</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           NEIN   Assets\n",
              "NEIN    1.00000  0.98751\n",
              "Assets  0.98751  1.00000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnmneDKvBdaz"
      },
      "source": [
        "As we can see that NEIN and Assets are highly correlated.\n",
        "\n",
        "What are we going to do to understand (1.1)?\n",
        "\n",
        "- Let's first regress NEIN on Hours      \n",
        "- Then see if the slope is significant.\n",
        "- If significant let's compute the residual.\n",
        "- Then let's regress Assets on the residual\n",
        "- Check for the significance.\n",
        "\n",
        "What we are hoping to see is that the residual-Assets slope will be insignificant as there is very less information in Assets that we can use to explain Hours using a linear model. Why? Because Assets-NEIN are correlated. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XrC0ku0BXNv",
        "outputId": "09887cac-f846-445f-cddf-ea23d66ebad2"
      },
      "source": [
        "Y=df.Hours.to_numpy()\n",
        "X1=df.NEIN.to_numpy()\n",
        "X2=df.Assets.to_numpy()\n",
        "regression=udregression\n",
        "regression(X1,Y).print_result()"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "var           coef    st error    t values     p values           LL         RL\n",
            "-----  -----------  ----------  ----------  -----------  -----------  ---------\n",
            "const  2033.84      21.0139       96.7856   4.35155e-42  1991.09      2076.59\n",
            "x1        0.318879   0.0599264     5.32117  7.15058e-06     0.196958     0.4408\n",
            "\n",
            "  rsquared    rsquared_Adj    fvalue\n",
            "----------  --------------  --------\n",
            "  0.461794        0.445485   28.3149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ditti-sC6Pu"
      },
      "source": [
        "From the above results we can observe that the slopes are significant. Now let's compute the residuals and finish the above procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjhjY-RRC1jq",
        "outputId": "91c29ff5-c621-403c-8c63-84d15c8f16e3"
      },
      "source": [
        "Residual=regression(X1,Y).fit_model().predict()-Y\n",
        "regression(X2,Residual) #Regressing Assets with the residual\n",
        "#let's check the results\n",
        "regression(X2,Residual).print_result()"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "var           coef     st error    t values    p values            LL           RL\n",
            "-----  -----------  -----------  ----------  ----------  ------------  -----------\n",
            "const   6.56741     19.0888        0.344045    0.732995  -32.269       45.4039\n",
            "x1     -0.00108075   0.00284811   -0.379463    0.706775   -0.00687527   0.00471377\n",
            "\n",
            "  rsquared    rsquared_Adj    fvalue\n",
            "----------  --------------  --------\n",
            "0.00434444      -0.0258269  0.143992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1titc6SDos6"
      },
      "source": [
        "Now we can see that the slopes are insignificant. One can also reverse X1 with X2 and conclude the same. Now the question is what can we do about this problem. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM-Ytvk6Eqyi"
      },
      "source": [
        "### Section 2\n",
        "\n",
        "Suppose we can create $z_1$ and $z_2$ such that    \n",
        "- $z_1=a_{11}x_1+a_{12}x_2$ and $z_2=a_{21}x_1+a_{22}x_2$\n",
        "- $V(z_{1})>V(z_{2})$ \n",
        "- z1 and z2 are ortho normal vectors\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7nu5-BHIZWn"
      },
      "source": [
        "**How to interpret the above conditions?**    \n",
        "We must interpret (1.1) and the adjoining example in a practical way. (1.1) is essentially telling us that we can do our analysis with just one variable instead of two. But that does not mean that we can abandon x1 or x2 with out incurring any error because x1 and x2 may be present in our model because of some empirical considerations. So when we extract z1 and z2 we are cleverly partitioning the variance thereby extracting orthogonal vectors that do not share any information with one another. This can be seen clearly when we generalise this derivation to a p variate problem.    \n",
        "Now if the variance of z2 is very less we can omit it and do our analysis with just z1."
      ]
    }
  ]
}