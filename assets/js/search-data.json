{
  
    
        "post0": {
            "title": "temper2",
            "content": "import numpy as np . def gram_schmidt_columns(X): Q, R = np.linalg.qr(X) return Q . def gs(X, row_vecs=True, norm = True): if not row_vecs: X = X.T Y = X[0:1,:].copy() for i in range(1, X.shape[0]): proj = np.diag((X[i,:].dot(Y.T)/np.linalg.norm(Y,axis=1)**2).flat).dot(Y) Y = np.vstack((Y, X[i,:] - proj.sum(0))) if norm: Y = np.diag(1/np.linalg.norm(Y,axis=1)).dot(Y) if row_vecs: return Y else: return Y.T . import numpy def gs_cofficient(v1, v2): return numpy.dot(v2, v1) / numpy.dot(v1, v1) def multiply(cofficient, v): return map((lambda x : x * cofficient), v) def proj(v1, v2): return multiply(gs_cofficient(v1, v2) , v1) def gs(X): Y = [] for i in range(len(X)): temp_vec = X[i] for inY in Y : proj_vec = proj(inY, X[i]) #print &quot;i =&quot;, i, &quot;, projection vector =&quot;, proj_vec temp_vec = map(lambda x, y : x - y, temp_vec, proj_vec) #print &quot;i =&quot;, i, &quot;, temporary vector =&quot;, temp_vec Y.append(temp_vec) return Y test = numpy.array([[3.0, 1.0], [2.0, 2.0]]) test2 = numpy.array([[1.0, 1.0, 0.0], [1.0, 3.0, 1.0], [2.0, -1.0, 1.0]]) print numpy.array(gs(test)) print numpy.array(gs(test2)) . CRAMERS RULE if __name__ == &quot;__main__&quot;: varA, varB, varC, varD, varE, varF = map( float, input(&quot;Enter a, b, c, d, e, f: &quot;).split(&quot;,&quot;)) if ((varA * varD) - (varB * varC)) == 0: print(&quot;The equation has no solution&quot;) else: solutionX = ((varE * varD) - (varB * varF)) / ((varA * varD) - (varB * varC)) solutionY = ((varA * varF) - (varE * varC)) / ((varA * varD) - (varB * varC)) print(&quot;x is &quot; + str(solutionX) + &quot; and y is &quot; + str(solutionY)) @ . Eigen values and Eigen vectors . Learning objectives . To define eigen values and vectors. | List a few properties which we will learn in detail in the future modules | Using python, how to find eigen values and vectors. | . Let A be a matrix and x be a vector. How to interpret Ax? If Ax is possible then Ax is a vector in a space spanned by the columns of A(column space of A). So another way to look at it is that A transforms vector x (i.e., change of scale, direction) into a vector y (=Ax) which lies in the column space of A. . Eigen vectors are a special type of vectors where they don&#39;t change direction but their length changes when multiplied with A OR Eigen vectors are those vectors that satisfies the equation $Ax= lambda x$ where $ lambda$ can be a fraction, any integer etc. The number $ lambda$ is the eigen value. . Note- Let $A = I$ where $I$ is the identity matrix. For A, all vector x is an Eigen vector with eigen value = 1. . Example . Lets illustrate the above definition using the example below . $Ax = begin{pmatrix} .8 &amp; .3 .2 &amp; .7 end{pmatrix}x= lambda x$ | Here to extract the eigen values we must solve the following equation. . $(A- lambda I)x=0 $ if the eigen value exist then the determinat of $A- lambda I$ must be equal to zero. (why?) . Therefore the equation we must solve here (after all simplification) equals $2 lambda ^2-3 lambda +1 =0$ , we get $ lambda = 1,1/2$ and substituting for $ lambda$ in 1 we can extract the corresponding eigen vectors and they are . $x= begin{pmatrix}.6 .4 end{pmatrix}, lambda = 1$ &amp; $x= begin{pmatrix}1 -1 end{pmatrix}, lambda = 1/2$ . Let us think about the geometry of eigen vectors. when x is multiplied with A the eigen vector with $ lambda$ value = 1 stays the same in direction and in length but for $ lambda$ = 1/2 that eigen vectors magnitude halves. . Elimination does not perserve the $ lambda$s | The product of n eigen values equals the determinant of the matrix | The sum of n eigen values is equal to the sum of the diagonal values of a matrix which is called the trace of the matrix. | One of the many applications of eigen values is that they help in computing large powers of a given matrix(what type of matrix?) i.e., Let&#39;s say $A^{100}$ | . How to find eigen values and vectors in python? . A=np.array([[1,2,1],[3,5,2],[4,3,4]]) print(A) . [[1 2 1] [3 5 2] [4 3 4]] . eigen_values,eigen_vectors=np.linalg.eig(A) . print(eigen_values) . [ 8.37685015 -0.30893252 1.93208237] . print(eigen_vectors) # i th row represent the eigen vector of the respective # i th eigen value . [[-0.27355094 -0.78788511 -0.09183165] [-0.65834893 0.23001831 -0.47923262] [-0.70124644 0.57125181 0.87287058]] . Trace=A[0][0]+A[1][1]+A[2][2] sum_of_eigen_values = round(np.sum(eigen_values),1) print(&quot;trace of the matrix A = &quot;,float(Trace)) print(&quot;Sum of the eigen values of A = &quot;,sum_of_eigen_values) . trace of the matrix A = 10.0 Sum of the eigen values of A = 10.0 . To show that elimination does not perserve $ lambda$s. . Let P be a permutation matrix. P reduces row 2 by subtracting the values in row 2 with the corresponding values in row 1 multiplied by 3. . P=np.array([[1,0,0],[-3,1,0],[0,0,1]]) . print(P@A) . [[ 1 2 1] [ 0 -1 -1] [ 4 3 4]] . B=P@A . eig_values_B,eig_vectors=np.linalg.eig(B) . print(&quot;eigen values of A after elimination = &quot;,eig_values_B) print(&quot;eigen values of A bfore elimination = &quot;,eigen_values) . eigen values of A after elimination = [ 4.1925824 1. -1.1925824] eigen values of A bfore elimination = [ 8.37685015 -0.30893252 1.93208237] . $(A- lambda I)x=0 $ if the eigen value exist then the determinat of $A- lambda I$ must be equal to zero. (why?) . Basically, if there exist a x other than the trivial zero/null vector it means that the null space spanned by $A- lambda I$ contains vectors other than the null vector and also implies that the vectors sitting in the columns of $A- lambda I$ are dependent which means the matrix don&#39;t have full column rank which implies that determinant of $A- lambda I$ is equal to zero. . Summary . An eigenvector or characteristic vector of a linear transformation (read Matrix for now) is a nonzero vector that changes at most by a scalar factor when that linear transformation is applied to it. | The sum and product of the A&#39;s equal the trace and delerminant of A respectively | Elimination does not perserve the λs | . Note The eigenvalues of $A^2$ &amp; $A^{-1}$ are $ lambda ^2$ &amp; $ lambda ^{-1}$ respectively .",
            "url": "https://varadan13.github.io/import_this/tutorial/2021/03/26/tete.html",
            "relUrl": "/tutorial/2021/03/26/tete.html",
            "date": " • Mar 26, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "temper",
            "content": "def gram_schmidt_columns(X): Q, R = np.linalg.qr(X) return Q . def gs(X, row_vecs=True, norm = True): if not row_vecs: X = X.T Y = X[0:1,:].copy() for i in range(1, X.shape[0]): proj = np.diag((X[i,:].dot(Y.T)/np.linalg.norm(Y,axis=1)**2).flat).dot(Y) Y = np.vstack((Y, X[i,:] - proj.sum(0))) if norm: Y = np.diag(1/np.linalg.norm(Y,axis=1)).dot(Y) if row_vecs: return Y else: return Y.T . import numpy def gs_cofficient(v1, v2): return numpy.dot(v2, v1) / numpy.dot(v1, v1) def multiply(cofficient, v): return map((lambda x : x * cofficient), v) def proj(v1, v2): return multiply(gs_cofficient(v1, v2) , v1) def gs(X): Y = [] for i in range(len(X)): temp_vec = X[i] for inY in Y : proj_vec = proj(inY, X[i]) #print &quot;i =&quot;, i, &quot;, projection vector =&quot;, proj_vec temp_vec = map(lambda x, y : x - y, temp_vec, proj_vec) #print &quot;i =&quot;, i, &quot;, temporary vector =&quot;, temp_vec Y.append(temp_vec) return Y test = numpy.array([[3.0, 1.0], [2.0, 2.0]]) test2 = numpy.array([[1.0, 1.0, 0.0], [1.0, 3.0, 1.0], [2.0, -1.0, 1.0]]) print numpy.array(gs(test)) print numpy.array(gs(test2)) . CRAMERS RULE if __name__ == &quot;__main__&quot;: varA, varB, varC, varD, varE, varF = map( float, input(&quot;Enter a, b, c, d, e, f: &quot;).split(&quot;,&quot;)) if ((varA * varD) - (varB * varC)) == 0: print(&quot;The equation has no solution&quot;) else: solutionX = ((varE * varD) - (varB * varF)) / ((varA * varD) - (varB * varC)) solutionY = ((varA * varF) - (varE * varC)) / ((varA * varD) - (varB * varC)) print(&quot;x is &quot; + str(solutionX) + &quot; and y is &quot; + str(solutionY)) @ . Eigen values and Eigen vectors . Let A be a matrix and x be a vector. How to interpret Ax? If Ax is possible then Ax is a vector in a space spanned by the columns of A(column space of A). So another way to look at it is that A transforms vector x (i.e., change of scale, direction) into a vector y (=Ax) which lies in the column space of A. . Eigen vectors are a special type of vectors where they don&#39;t change direction but their length changes when multiplied with A. Or Eigen vectors are those vectors that satisfies the equation $Ax= lambda x$ where $ lambda$ can be a fraction, any integer etc. The number $ lambda$ is the eigen value. . Let $A = I$ where $I$ is the identity matrix. For A all vector X is an Eigen Value .",
            "url": "https://varadan13.github.io/import_this/tutorial/2021/03/26/hlll.html",
            "relUrl": "/tutorial/2021/03/26/hlll.html",
            "date": " • Mar 26, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Eigen values and Eigen vectors",
            "content": "Learning objectives . To define eigen values and vectors and state some of their properties | To factorize a square matrix $A$ into $S Lambda S^{-1}$. | Factorizing a symmetric matrix | Spectral theorem | Positive definite matrices | Singular value decomposition | . Section 1: Basics . Let A be a matrix and x be a vector. How to interpret Ax? If Ax is possible then Ax is a vector in a space spanned by the columns of A(column space of A). So another way to look at it is that A transforms vector x (i.e., change of scale, direction) into a vector y (=Ax) which lies in the column space of A. . Eigen vectors are a special type of vectors where they don&#39;t change direction but their length changes when multiplied with A OR Eigen vectors are those vectors that satisfies the equation $Ax= lambda x$ where $ lambda$ can be a fraction, any integer etc. The number $ lambda$ is the eigen value. . Note- Let $A = I$ where $I$ is the identity matrix. For A, all vector x is an Eigen vector with eigen value = 1. . Example . Lets illustrate the above definition using the example below . $Ax = begin{pmatrix} .8 &amp; .3 .2 &amp; .7 end{pmatrix}x= lambda x$ | Here to extract the eigen values we must solve the following equation. . $(A- lambda I)x=0 $ if the eigen value exist then the determinat of $A- lambda I$ must be equal to zero. (why?) . Therefore the equation we must solve here (after all simplification) equals $2 lambda ^2-3 lambda +1 =0$ , we get $ lambda = 1,1/2$ and substituting for $ lambda$ in 1 we can extract the corresponding eigen vectors and they are . $x= begin{pmatrix}.6 .4 end{pmatrix}, lambda = 1$ &amp; $x= begin{pmatrix}1 -1 end{pmatrix}, lambda = 1/2$ . Let us think about the geometry of eigen vectors. when x is multiplied with A the eigen vector with $ lambda$ value = 1 stays the same in direction and in length but for $ lambda$ = 1/2 that eigen vectors magnitude halves. . Elimination does not perserve the $ lambda$s | The product of n eigen values equals the determinant of the matrix | The sum of n eigen values is equal to the sum of the diagonal values of a matrix which is called the trace of the matrix. | One of the many applications of eigen values is that they help in computing large powers of a given matrix(what type of matrix?) i.e., Let&#39;s say $A^{100}$ | . Finding eigen values and vectors in python . import numpy as np . A=np.array([[1,2,1],[3,5,2],[4,3,4]]) print(A) . [[1 2 1] [3 5 2] [4 3 4]] . eigen_values,eigen_vectors=np.linalg.eig(A) #the obtained eigen vectors are unit vectors. . print(eigen_values) . [ 8.37685015 -0.30893252 1.93208237] . print(eigen_vectors) # i th row represent the eigen vector of the respective # i th eigen value . [[-0.27355094 -0.78788511 -0.09183165] [-0.65834893 0.23001831 -0.47923262] [-0.70124644 0.57125181 0.87287058]] . Trace=A[0][0]+A[1][1]+A[2][2] sum_of_eigen_values = round(np.sum(eigen_values),1) print(&quot;trace of the matrix A = &quot;,float(Trace)) print(&quot;Sum of the eigen values of A = &quot;,sum_of_eigen_values) . trace of the matrix A = 10.0 Sum of the eigen values of A = 10.0 . To show that elimination does not perserve $ lambda$s. . Let P be a permutation matrix. P reduces row 2 by subtracting the values in row 2 with the corresponding values in row 1 multiplied by 3. . P=np.array([[1,0,0],[-3,1,0],[0,0,1]]) . print(P@A) . [[ 1 2 1] [ 0 -1 -1] [ 4 3 4]] . B=P@A . eig_values_B,eig_vectors=np.linalg.eig(B) . print(&quot;eigen values of A after elimination = &quot;,eig_values_B) print(&quot;eigen values of A bfore elimination = &quot;,eigen_values) . eigen values of A after elimination = [ 4.1925824 1. -1.1925824] eigen values of A bfore elimination = [ 8.37685015 -0.30893252 1.93208237] . $(A- lambda I)x=0 $ if the eigen value exist then the determinat of $A- lambda I$ must be equal to zero. (why?) . Basically, if there exist a x other than the trivial zero/null vector it means that the null space spanned by $A- lambda I$ contains vectors other than the null vector and also implies that the vectors sitting in the columns of $A- lambda I$ are dependent which means the matrix don&#39;t have full column rank which implies that determinant of $A- lambda I$ is equal to zero. . Summary . An eigenvector or characteristic vector of a linear transformation (read Matrix for now) is a nonzero vector that changes at most by a scalar factor when that linear transformation is applied to it. | The sum and product of the A&#39;s equal the trace and delerminant of A respectively | Elimination does not perserve the λs | . Note The eigenvalues of $A^2$ &amp; $A^{-1}$ are $ lambda ^2$ &amp; $ lambda ^{-1}$ respectively . Note if x is an eigen vector of A then Ax is simply reduced to $ lambda x$. Similarly, AAx=$ lambda lambda x$. We can generalise it as $A^{n}x= lambda ^{n}x$ where x is an eigen vector . Section 2 : Diagonalizing a matrix using Eigen vectors . (2.1) Suppose the n by n matrix A has n linearly independent eigen vectors $x_1,x_2,...,x_n$ and S be a matrix whose columns contains these n independent eigen vectors then $S^{-1}AS = Lambda$ where $ Lambda$ is eigen value matrix of A and also diagonalized matrix of A. . What is the Eigen Value Matrix of A? A matrix that contains eigen values of A in its diagonal position with off diagonal entry equals to zero. . Let&#39;s use (2.1) to rewrite A in a more meaningful way. From (2.1) $S^{-1}AS = Lambda$ which implies that $AS=S Lambda$ and this can be written as $A=S Lambda S^{-1}$. . (2.2) $A=S Lambda S^{-1}$ iff eigen vectors are linearly independent. OR in other words all the eigen vectors are unique. Matrices with repeated eigen values cannot be factorised into this form because S will not be invertible(why?) . Note S is not a unique matrix (why?) . What is the meaning of (2.2)? How can we use it? Let A be a matrix that can be factorised into (2.2) now it becomes easy to compute $A^{n}$ because $A^{n}=S Lambda^{n} S^{-1}$ . def check_uniqueness(eigen_value): checker=[i for i in range(0,len(eigen_value)-1) for j in range(i+1,len(eigen_value)) if eigen_value[i]==eigen_value[j]] if len(checker)==0: return &#39;UNIQUE&#39; else: return &#39;NOT UNIQUE&#39; . def diagonalizer(A): eigen_value,w=np.linalg.eig(A) if check_uniqueness(eigen_value)==&#39;UNIQUE&#39;: return np.round(np.linalg.inv(w)@(A@w),3) else: print(&quot;Sorry the matrix does not have unique eigen values&quot;) . A=np.array([[1,2,1],[3,5,2],[4,3,4]]) e,_=np.linalg.eig(A) print(diagonalizer(A)) print() print(&quot;the eigen values of the Matrix A = &quot;, e) . [[ 8.377 -0. -0. ] [ 0. -0.309 0. ] [ 0. 0. 1.932]] the eigen values of the Matrix A = [ 8.37685015 -0.30893252 1.93208237] . Setion 3:Diagonalizing a symmetric matrix . 3.1 Definition A is a symmetric matrix if $A=A^{T}$ . Let&#39;s explore using an example . $sys = begin{pmatrix} 5 &amp; 6 &amp; 7 6 &amp; 3 &amp; 2 7 &amp; 2 &amp; 1 end{pmatrix}$ we can see that sys is a 3*3 symmetric matrix. . sys = np.array([[5,6,7],[6,3,2],[7,2,1]]) # Let&#39;s check if the eigen values of sys matrix is unique or not eig,_=np.linalg.eig(sys) check_uniqueness(eig)==&#39;UNIQUE&#39; . True . Therefore eigen values of sys matrix is unique. . Let&#39;s check out the eigen vector matrix of sys . eig,S=np.linalg.eig(sys) print(&quot;the eigen vector matrix of sys = n&quot;, S) . the eigen vector matrix of sys = [[ 0.72541697 0.66666667 0.17124772] [ 0.49566405 -0.33333333 -0.80200127] [ 0.47758494 -0.66666667 0.57224835]] . What are the properties of S? . eig_vec1,eig_vec2,eig_vec3=S[:,0],S[:,1],S[:,2] . round(np.dot(eig_vec1,eig_vec2),3) . -0.0 . round(np.dot(eig_vec2,eig_vec3),3) . 0.0 . round(np.dot(eig_vec3,eig_vec1),3) . -0.0 . The first thing therefore we notice is that the eigen vectors are orthogonal to each other . np.round(S@S.T,3) . array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) . np.round(S.T@S,3) . array([[ 1., 0., -0.], [ 0., 1., 0.], [-0., 0., 1.]]) . The second thing we notice is that the inverse of eigen vector matrix is its transpose therefore eigen vector matrix of a symmetric matrix is an orthogonal matrix . (3.2) Definition Orthogonal matrix An orthogonal matrix is a real square matrix whose columns and rows are orthogonal vectors OR $M*M^{-1}=I$ where M is the orthogonal matrix. . Here, the eigen vector matrix is not only orthogonal but also orthonormal because numpy normalises the obtained eigen vectors into unit vectors. . For example:- . round(np.dot(eig_vec1,eig_vec1),3) . 1.0 . What are the properties of S? i.e., eigen vector matrix of a symmetric matrix . eigen vectors are orthogonal to each other | the inverse of eigen vector matrix is its transpose therefore eigen vector matrix of a symmetric matrix is an orthogonal matrix | if the eigen vectors are unit vectors then the eigen vector matrix is an orthonormal matrix. | . Now let&#39;s answer the question, what is the form of matrix diagonalisation when the matrix is symmetric? . $S^{-1}AS= Lambda$ this is when A is non symmetric when A is symmetric the equation becomes $S^{T}AS= Lambda$ . This journey culminates with the spectral theorem . (3.3) Spectral theorem Every symmetric matrix A can be factorised as $S Lambda S^{T}$ with $ Lambda$ matrix holding real eigen values in its diagonal position and orthonormal eigen vectors sitting in the columns of S matrix. . Summary of Section 2 and 3 . We have learnt that A can be factorised into $A=S Lambda S^{-1}$ | This factorisation can be used to seamlessly compute $A^{n}$ by using the following formula $A^{n}=S Lambda^{n} S^{-1}$. | . Section 4: Positive definite &amp; Singular Value Decomposition . (4.1) Definition Positive definite matrix A symmetric matrix whose eigen values are all positive . How to test for positive definitness? The matrix A is positive definite if $x^{T}Ax&gt;0$ for all nonzero vector X .",
            "url": "https://varadan13.github.io/import_this/linear%20algebra%20tutorial/2021/03/26/EIGEN-VALUE.html",
            "relUrl": "/linear%20algebra%20tutorial/2021/03/26/EIGEN-VALUE.html",
            "date": " • Mar 26, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Archive of Metaphors and thoughts, March 2021",
            "content": "I love metaphors and surreal thoughts because I believe they are like mathematics highly capable of telling so many things in a sentence or two. I will maintain this archive as long as I find time to read some of my favourite authors and poets. . . “Tell me, is the rose naked or is that her only dress?” . “How many churches are there in heaven?” . “Why do leaves commit suicide when they feel yellow?” . “How many questions does a cat have?” . “What will they say about my poetry who never touched my blood?” . “And who asked spring time for its kingdom of clear air?” . “Why do all silkworms live so raggedly?” . Pablo Nerudo, Book of Questions . “I never hear the word “escape” Without a quicker blood, A sudden expectation, A flying attItude’” . Emily Dickinson . “Nameless I came among olives of algae, Foetus of plankton, I remember nothing.” . “Like gommiers loosened from Guinea, Far from the childhood of rivers” . Derek Walcott, Origins . “The designs of the universe are unknown to us, but we do know that to think with lucidity and to act with fairness is to aid those de­signs (which shall never be revealed to us).” . Jorge Luis Borges, A Prayer. . “Passion, after all, comes from the Latin for suffering.” . The dispassionate developer by Mark Seeman . “26 Years In The Deepest Darkest Jungle And Still I Became My Father” . Robin Williams in Jumanji . “Where some people have a self, most people have a void, because they are too busy in wasting their vital creative energy to project themselves as this or that… actualizing a concept of what they should be like rather than actualizing their potentiality as a human being” . “when it comes to observing faults in others, most of us are are quick to react with condemnation. But what about looking inwardly for a change? To personally examine who we really are and what we are, our merits as well as our faults — in short, to see oneself as [one] is for once and to take responsibility [for] oneself.” . “I am happy because I am daily growing and honestly not knowing where the limit will yet lie.” . “I don’t believe in the manipulation game of creating a self image robot.” . Bruce Lee .",
            "url": "https://varadan13.github.io/import_this/metaphor%20archive/2021/03/25/March-metaphors.html",
            "relUrl": "/metaphor%20archive/2021/03/25/March-metaphors.html",
            "date": " • Mar 25, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://varadan13.github.io/import_this/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://varadan13.github.io/import_this/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://varadan13.github.io/import_this/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://varadan13.github.io/import_this/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}