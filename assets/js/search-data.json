{
  
    
        "post0": {
            "title": "temper2",
            "content": "import numpy as np . def gram_schmidt_columns(X): Q, R = np.linalg.qr(X) return Q . def gs(X, row_vecs=True, norm = True): if not row_vecs: X = X.T Y = X[0:1,:].copy() for i in range(1, X.shape[0]): proj = np.diag((X[i,:].dot(Y.T)/np.linalg.norm(Y,axis=1)**2).flat).dot(Y) Y = np.vstack((Y, X[i,:] - proj.sum(0))) if norm: Y = np.diag(1/np.linalg.norm(Y,axis=1)).dot(Y) if row_vecs: return Y else: return Y.T . import numpy def gs_cofficient(v1, v2): return numpy.dot(v2, v1) / numpy.dot(v1, v1) def multiply(cofficient, v): return map((lambda x : x * cofficient), v) def proj(v1, v2): return multiply(gs_cofficient(v1, v2) , v1) def gs(X): Y = [] for i in range(len(X)): temp_vec = X[i] for inY in Y : proj_vec = proj(inY, X[i]) #print &quot;i =&quot;, i, &quot;, projection vector =&quot;, proj_vec temp_vec = map(lambda x, y : x - y, temp_vec, proj_vec) #print &quot;i =&quot;, i, &quot;, temporary vector =&quot;, temp_vec Y.append(temp_vec) return Y test = numpy.array([[3.0, 1.0], [2.0, 2.0]]) test2 = numpy.array([[1.0, 1.0, 0.0], [1.0, 3.0, 1.0], [2.0, -1.0, 1.0]]) print numpy.array(gs(test)) print numpy.array(gs(test2)) . CRAMERS RULE if __name__ == &quot;__main__&quot;: varA, varB, varC, varD, varE, varF = map( float, input(&quot;Enter a, b, c, d, e, f: &quot;).split(&quot;,&quot;)) if ((varA * varD) - (varB * varC)) == 0: print(&quot;The equation has no solution&quot;) else: solutionX = ((varE * varD) - (varB * varF)) / ((varA * varD) - (varB * varC)) solutionY = ((varA * varF) - (varE * varC)) / ((varA * varD) - (varB * varC)) print(&quot;x is &quot; + str(solutionX) + &quot; and y is &quot; + str(solutionY)) @ . Eigen values and Eigen vectors . Learning objectives . To define eigen values and vectors. | List a few properties which we will learn in detail in the future modules | Using python, how to find eigen values and vectors. | . Let A be a matrix and x be a vector. How to interpret Ax? If Ax is possible then Ax is a vector in a space spanned by the columns of A(column space of A). So another way to look at it is that A transforms vector x (i.e., change of scale, direction) into a vector y (=Ax) which lies in the column space of A. . Eigen vectors are a special type of vectors where they don&#39;t change direction but their length changes when multiplied with A OR Eigen vectors are those vectors that satisfies the equation $Ax= lambda x$ where $ lambda$ can be a fraction, any integer etc. The number $ lambda$ is the eigen value. . Note- Let $A = I$ where $I$ is the identity matrix. For A, all vector x is an Eigen vector with eigen value = 1. . Example . Lets illustrate the above definition using the example below . $Ax = begin{pmatrix} .8 &amp; .3 .2 &amp; .7 end{pmatrix}x= lambda x$ | Here to extract the eigen values we must solve the following equation. . $(A- lambda I)x=0 $ if the eigen value exist then the determinat of $A- lambda I$ must be equal to zero. (why?) . Therefore the equation we must solve here (after all simplification) equals $2 lambda ^2-3 lambda +1 =0$ , we get $ lambda = 1,1/2$ and substituting for $ lambda$ in 1 we can extract the corresponding eigen vectors and they are . $x= begin{pmatrix}.6 .4 end{pmatrix}, lambda = 1$ &amp; $x= begin{pmatrix}1 -1 end{pmatrix}, lambda = 1/2$ . Let us think about the geometry of eigen vectors. when x is multiplied with A the eigen vector with $ lambda$ value = 1 stays the same in direction and in length but for $ lambda$ = 1/2 that eigen vectors magnitude halves. . Elimination does not perserve the $ lambda$s | The product of n eigen values equals the determinant of the matrix | The sum of n eigen values is equal to the sum of the diagonal values of a matrix which is called the trace of the matrix. | One of the many applications of eigen values is that they help in computing large powers of a given matrix(what type of matrix?) i.e., Let&#39;s say $A^{100}$ | . How to find eigen values and vectors in python? . A=np.array([[1,2,1],[3,5,2],[4,3,4]]) print(A) . [[1 2 1] [3 5 2] [4 3 4]] . eigen_values,eigen_vectors=np.linalg.eig(A) . print(eigen_values) . [ 8.37685015 -0.30893252 1.93208237] . print(eigen_vectors) # i th row represent the eigen vector of the respective # i th eigen value . [[-0.27355094 -0.78788511 -0.09183165] [-0.65834893 0.23001831 -0.47923262] [-0.70124644 0.57125181 0.87287058]] . Trace=A[0][0]+A[1][1]+A[2][2] sum_of_eigen_values = round(np.sum(eigen_values),1) print(&quot;trace of the matrix A = &quot;,float(Trace)) print(&quot;Sum of the eigen values of A = &quot;,sum_of_eigen_values) . trace of the matrix A = 10.0 Sum of the eigen values of A = 10.0 . To show that elimination does not perserve $ lambda$s. . Let P be a permutation matrix. P reduces row 2 by subtracting the values in row 2 with the corresponding values in row 1 multiplied by 3. . P=np.array([[1,0,0],[-3,1,0],[0,0,1]]) . print(P@A) . [[ 1 2 1] [ 0 -1 -1] [ 4 3 4]] . B=P@A . eig_values_B,eig_vectors=np.linalg.eig(B) . print(&quot;eigen values of A after elimination = &quot;,eig_values_B) print(&quot;eigen values of A bfore elimination = &quot;,eigen_values) . eigen values of A after elimination = [ 4.1925824 1. -1.1925824] eigen values of A bfore elimination = [ 8.37685015 -0.30893252 1.93208237] . $(A- lambda I)x=0 $ if the eigen value exist then the determinat of $A- lambda I$ must be equal to zero. (why?) . Basically, if there exist a x other than the trivial zero/null vector it means that the null space spanned by $A- lambda I$ contains vectors other than the null vector and also implies that the vectors sitting in the columns of $A- lambda I$ are dependent which means the matrix don&#39;t have full column rank which implies that determinant of $A- lambda I$ is equal to zero. . Summary . An eigenvector or characteristic vector of a linear transformation (read Matrix for now) is a nonzero vector that changes at most by a scalar factor when that linear transformation is applied to it. | The sum and product of the A&#39;s equal the trace and delerminant of A respectively | Elimination does not perserve the λs | . Note The eigenvalues of $A^2$ &amp; $A^{-1}$ are $ lambda ^2$ &amp; $ lambda ^{-1}$ respectively .",
            "url": "https://varadan13.github.io/import_this/tutorial/2021/03/26/tete.html",
            "relUrl": "/tutorial/2021/03/26/tete.html",
            "date": " • Mar 26, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "temper",
            "content": "def gram_schmidt_columns(X): Q, R = np.linalg.qr(X) return Q . def gs(X, row_vecs=True, norm = True): if not row_vecs: X = X.T Y = X[0:1,:].copy() for i in range(1, X.shape[0]): proj = np.diag((X[i,:].dot(Y.T)/np.linalg.norm(Y,axis=1)**2).flat).dot(Y) Y = np.vstack((Y, X[i,:] - proj.sum(0))) if norm: Y = np.diag(1/np.linalg.norm(Y,axis=1)).dot(Y) if row_vecs: return Y else: return Y.T . import numpy def gs_cofficient(v1, v2): return numpy.dot(v2, v1) / numpy.dot(v1, v1) def multiply(cofficient, v): return map((lambda x : x * cofficient), v) def proj(v1, v2): return multiply(gs_cofficient(v1, v2) , v1) def gs(X): Y = [] for i in range(len(X)): temp_vec = X[i] for inY in Y : proj_vec = proj(inY, X[i]) #print &quot;i =&quot;, i, &quot;, projection vector =&quot;, proj_vec temp_vec = map(lambda x, y : x - y, temp_vec, proj_vec) #print &quot;i =&quot;, i, &quot;, temporary vector =&quot;, temp_vec Y.append(temp_vec) return Y test = numpy.array([[3.0, 1.0], [2.0, 2.0]]) test2 = numpy.array([[1.0, 1.0, 0.0], [1.0, 3.0, 1.0], [2.0, -1.0, 1.0]]) print numpy.array(gs(test)) print numpy.array(gs(test2)) . CRAMERS RULE if __name__ == &quot;__main__&quot;: varA, varB, varC, varD, varE, varF = map( float, input(&quot;Enter a, b, c, d, e, f: &quot;).split(&quot;,&quot;)) if ((varA * varD) - (varB * varC)) == 0: print(&quot;The equation has no solution&quot;) else: solutionX = ((varE * varD) - (varB * varF)) / ((varA * varD) - (varB * varC)) solutionY = ((varA * varF) - (varE * varC)) / ((varA * varD) - (varB * varC)) print(&quot;x is &quot; + str(solutionX) + &quot; and y is &quot; + str(solutionY)) @ . Eigen values and Eigen vectors . Let A be a matrix and x be a vector. How to interpret Ax? If Ax is possible then Ax is a vector in a space spanned by the columns of A(column space of A). So another way to look at it is that A transforms vector x (i.e., change of scale, direction) into a vector y (=Ax) which lies in the column space of A. . Eigen vectors are a special type of vectors where they don&#39;t change direction but their length changes when multiplied with A. Or Eigen vectors are those vectors that satisfies the equation $Ax= lambda x$ where $ lambda$ can be a fraction, any integer etc. The number $ lambda$ is the eigen value. . Let $A = I$ where $I$ is the identity matrix. For A all vector X is an Eigen Value .",
            "url": "https://varadan13.github.io/import_this/tutorial/2021/03/26/hlll.html",
            "relUrl": "/tutorial/2021/03/26/hlll.html",
            "date": " • Mar 26, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Eigen values and Eigen vectors",
            "content": "Learning objectives . To define eigen values and vectors and state some of their properties | To factorize a square matrix $A$ into $S Lambda S^{-1}$. | Factorizing a symmetric matrix | Spectral theorem | Positive definite matrices | Singular value decomposition | . Section 1: Basics . Let A be a matrix and x be a vector. How to interpret Ax? If Ax is possible then Ax is a vector in a space spanned by the columns of A(column space of A). So another way to look at it is that A transforms vector x (i.e., change of scale, direction) into a vector y (=Ax) which lies in the column space of A. . Eigen vectors are a special type of vectors where they don&#39;t change direction but their length changes when multiplied with A OR Eigen vectors are those vectors that satisfies the equation $Ax= lambda x$ where $ lambda$ can be a fraction, any integer etc. The number $ lambda$ is the eigen value. . Note- Let $A = I$ where $I$ is the identity matrix. For A, all vector x is an Eigen vector with eigen value = 1. . Example . Lets illustrate the above definition using the example below . $Ax = begin{pmatrix} .8 &amp; .3 .2 &amp; .7 end{pmatrix}x= lambda x$ | Here to extract the eigen values we must solve the following equation. . $(A- lambda I)x=0 $ if the eigen value exist then the determinat of $A- lambda I$ must be equal to zero. (why?) . Therefore the equation we must solve here (after all simplification) equals $2 lambda ^2-3 lambda +1 =0$ , we get $ lambda = 1,1/2$ and substituting for $ lambda$ in 1 we can extract the corresponding eigen vectors and they are . $x= begin{pmatrix}.6 .4 end{pmatrix}, lambda = 1$ &amp; $x= begin{pmatrix}1 -1 end{pmatrix}, lambda = 1/2$ . Let us think about the geometry of eigen vectors. when x is multiplied with A the eigen vector with $ lambda$ value = 1 stays the same in direction and in length but for $ lambda$ = 1/2 that eigen vectors magnitude halves. . Elimination does not perserve the $ lambda$s | The product of n eigen values equals the determinant of the matrix | The sum of n eigen values is equal to the sum of the diagonal values of a matrix which is called the trace of the matrix. | One of the many applications of eigen values is that they help in computing large powers of a given matrix(what type of matrix?) i.e., Let&#39;s say $A^{100}$ | . Finding eigen values and vectors in python . import numpy as np . A=np.array([[1,2,1],[3,5,2],[4,3,4]]) print(A) . [[1 2 1] [3 5 2] [4 3 4]] . eigen_values,eigen_vectors=np.linalg.eig(A) #the obtained eigen vectors are unit vectors. . print(eigen_values) . [ 8.37685015 -0.30893252 1.93208237] . print(eigen_vectors) # i th row represent the eigen vector of the respective # i th eigen value . [[-0.27355094 -0.78788511 -0.09183165] [-0.65834893 0.23001831 -0.47923262] [-0.70124644 0.57125181 0.87287058]] . Trace=A[0][0]+A[1][1]+A[2][2] sum_of_eigen_values = round(np.sum(eigen_values),1) print(&quot;trace of the matrix A = &quot;,float(Trace)) print(&quot;Sum of the eigen values of A = &quot;,sum_of_eigen_values) . trace of the matrix A = 10.0 Sum of the eigen values of A = 10.0 . To show that elimination does not perserve $ lambda$s. . Let P be a permutation matrix. P reduces row 2 by subtracting the values in row 2 with the corresponding values in row 1 multiplied by 3. . P=np.array([[1,0,0],[-3,1,0],[0,0,1]]) . print(P@A) . [[ 1 2 1] [ 0 -1 -1] [ 4 3 4]] . B=P@A . eig_values_B,eig_vectors=np.linalg.eig(B) . print(&quot;eigen values of A after elimination = &quot;,eig_values_B) print(&quot;eigen values of A bfore elimination = &quot;,eigen_values) . eigen values of A after elimination = [ 4.1925824 1. -1.1925824] eigen values of A bfore elimination = [ 8.37685015 -0.30893252 1.93208237] . $(A- lambda I)x=0 $ if the eigen value exist then the determinat of $A- lambda I$ must be equal to zero. (why?) . Basically, if there exist a x other than the trivial zero/null vector it means that the null space spanned by $A- lambda I$ contains vectors other than the null vector and also implies that the vectors sitting in the columns of $A- lambda I$ are dependent which means the matrix don&#39;t have full column rank which implies that determinant of $A- lambda I$ is equal to zero. . Summary . An eigenvector or characteristic vector of a linear transformation (read Matrix for now) is a nonzero vector that changes at most by a scalar factor when that linear transformation is applied to it. | The sum and product of the A&#39;s equal the trace and delerminant of A respectively | Elimination does not perserve the λs | . Note The eigenvalues of $A^2$ &amp; $A^{-1}$ are $ lambda ^2$ &amp; $ lambda ^{-1}$ respectively . Note if x is an eigen vector of A then Ax is simply reduced to $ lambda x$. Similarly, AAx=$ lambda lambda x$. We can generalise it as $A^{n}x= lambda ^{n}x$ where x is an eigen vector . Section 2 : Diagonalizing a matrix using Eigen vectors . (2.1) Suppose the n by n matrix A has n linearly independent eigen vectors $x_1,x_2,...,x_n$ and S be a matrix whose columns contains these n independent eigen vectors then $S^{-1}AS = Lambda$ where $ Lambda$ is eigen value matrix of A and also diagonalized matrix of A. . What is the Eigen Value Matrix of A? A matrix that contains eigen values of A in its diagonal position with off diagonal entry equals to zero. . Let&#39;s use (2.1) to rewrite A in a more meaningful way. From (2.1) $S^{-1}AS = Lambda$ which implies that $AS=S Lambda$ and this can be written as $A=S Lambda S^{-1}$. . (2.2) $A=S Lambda S^{-1}$ iff eigen vectors are linearly independent. OR in other words all the eigen vectors are unique. Matrices with repeated eigen values cannot be factorised into this form because S will not be invertible(why?) . Note S is not a unique matrix (why?) . What is the meaning of (2.2)? How can we use it? Let A be a matrix that can be factorised into (2.2) now it becomes easy to compute $A^{n}$ because $A^{n}=S Lambda^{n} S^{-1}$ . def check_uniqueness(eigen_value): checker=[i for i in range(0,len(eigen_value)-1) for j in range(i+1,len(eigen_value)) if eigen_value[i]==eigen_value[j]] if len(checker)==0: return &#39;UNIQUE&#39; else: return &#39;NOT UNIQUE&#39; . def diagonalizer(A): eigen_value,w=np.linalg.eig(A) if check_uniqueness(eigen_value)==&#39;UNIQUE&#39;: return np.round(np.linalg.inv(w)@(A@w),3) else: print(&quot;Sorry the matrix does not have unique eigen values&quot;) . A=np.array([[1,2,1],[3,5,2],[4,3,4]]) e,_=np.linalg.eig(A) print(diagonalizer(A)) print() print(&quot;the eigen values of the Matrix A = &quot;, e) . [[ 8.377 -0. -0. ] [ 0. -0.309 0. ] [ 0. 0. 1.932]] the eigen values of the Matrix A = [ 8.37685015 -0.30893252 1.93208237] . Setion 3:Diagonalizing a symmetric matrix . 3.1 Definition A is a symmetric matrix if $A=A^{T}$ . Let&#39;s explore using an example . $sys = begin{pmatrix} 5 &amp; 6 &amp; 7 6 &amp; 3 &amp; 2 7 &amp; 2 &amp; 1 end{pmatrix}$ we can see that sys is a 3*3 symmetric matrix. . sys = np.array([[5,6,7],[6,3,2],[7,2,1]]) # Let&#39;s check if the eigen values of sys matrix is unique or not eig,_=np.linalg.eig(sys) check_uniqueness(eig)==&#39;UNIQUE&#39; . True . Therefore eigen values of sys matrix is unique. . Let&#39;s check out the eigen vector matrix of sys . eig,S=np.linalg.eig(sys) print(&quot;the eigen vector matrix of sys = n&quot;, S) . the eigen vector matrix of sys = [[ 0.72541697 0.66666667 0.17124772] [ 0.49566405 -0.33333333 -0.80200127] [ 0.47758494 -0.66666667 0.57224835]] . What are the properties of S? . eig_vec1,eig_vec2,eig_vec3=S[:,0],S[:,1],S[:,2] . round(np.dot(eig_vec1,eig_vec2),3) . -0.0 . round(np.dot(eig_vec2,eig_vec3),3) . 0.0 . round(np.dot(eig_vec3,eig_vec1),3) . -0.0 . The first thing therefore we notice is that the eigen vectors are orthogonal to each other . np.round(S@S.T,3) . array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) . np.round(S.T@S,3) . array([[ 1., 0., -0.], [ 0., 1., 0.], [-0., 0., 1.]]) . The second thing we notice is that the inverse of eigen vector matrix is its transpose therefore eigen vector matrix of a symmetric matrix is an orthogonal matrix . (3.2) Definition Orthogonal matrix An orthogonal matrix is a real square matrix whose columns and rows are orthogonal vectors OR $M*M^{-1}=I$ where M is the orthogonal matrix. . Here, the eigen vector matrix is not only orthogonal but also orthonormal because numpy normalises the obtained eigen vectors into unit vectors. . For example:- . round(np.dot(eig_vec1,eig_vec1),3) . 1.0 . What are the properties of S? i.e., eigen vector matrix of a symmetric matrix . eigen vectors are orthogonal to each other | the inverse of eigen vector matrix is its transpose therefore eigen vector matrix of a symmetric matrix is an orthogonal matrix | if the eigen vectors are unit vectors then the eigen vector matrix is an orthonormal matrix. | . Now let&#39;s answer the question, what is the form of matrix diagonalisation when the matrix is symmetric? . $S^{-1}AS= Lambda$ this is when A is non symmetric when A is symmetric the equation becomes $S^{T}AS= Lambda$ . This journey culminates with the spectral theorem . (3.3) Spectral theorem Every symmetric matrix A can be factorised as $S Lambda S^{T}$ with $ Lambda$ matrix holding real eigen values in its diagonal position and orthonormal eigen vectors sitting in the columns of S matrix. . Summary of Section 2 and 3 . We have learnt that A can be factorised into $A=S Lambda S^{-1}$ | This factorisation can be used to seamlessly compute $A^{n}$ by using the following formula $A^{n}=S Lambda^{n} S^{-1}$. | . Section 4: Positive definite &amp; Singular Value Decomposition . (4.1) Definition Positive definite matrix A symmetric matrix whose eigen values are all positive . How to test for positive definitness? The matrix A is positive definite if $x^{T}Ax&gt;0$ for all nonzero vector X .",
            "url": "https://varadan13.github.io/import_this/linear%20algebra%20tutorial/2021/03/26/EIGEN-VALUE.html",
            "relUrl": "/linear%20algebra%20tutorial/2021/03/26/EIGEN-VALUE.html",
            "date": " • Mar 26, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Archive of Metaphors and thoughts, March 2021",
            "content": "I love metaphors and surreal thoughts because I believe they are like mathematics highly capable of telling so many things in a sentence or two. I will maintain this archive as long as I find time to read some of my favourite authors and poets. . . “Tell me, is the rose naked or is that her only dress?” . “How many churches are there in heaven?” . “Why do leaves commit suicide when they feel yellow?” . “How many questions does a cat have?” . “What will they say about my poetry who never touched my blood?” . “And who asked spring time for its kingdom of clear air?” . “Why do all silkworms live so raggedly?” . Pablo Nerudo, Book of Questions . “I never hear the word “escape” Without a quicker blood, A sudden expectation, A flying attItude’” . Emily Dickinson . “Nameless I came among olives of algae, Foetus of plankton, I remember nothing.” . “Like gommiers loosened from Guinea, Far from the childhood of rivers” . Derek Walcott, Origins . Close is what we almost always are: close to happiness, close to another, close to leaving, close to tears, close to God, close to losing faith, close to being done, close to saying something, or close to success, and even, with the greatest sense of satisfaction, close to giving the whole thing up. . Our human essence lies not in arrival, but in being almost there, we are creatures who are on the way, our journey a series of impending anticipated arrivals. We live by unconsciously measuring the inverse distances of our proximity: an intimacy calibrated by the vulnerability we feel in giving up our sense of separation. . David Wyte, Close . Inspired by David Whyte’s close a video titled ‘Proximity’. . “The designs of the universe are unknown to us, but we do know that to think with lucidity and to act with fairness is to aid those de­signs (which shall never be revealed to us).” . Jorge Luis Borges, A Prayer. . “Passion, after all, comes from the Latin for suffering.” . The dispassionate developer by Mark Seeman . “26 Years In The Deepest Darkest Jungle And Still I Became My Father” . Robin Williams in Jumanji . “Where some people have a self, most people have a void, because they are too busy in wasting their vital creative energy to project themselves as this or that… actualizing a concept of what they should be like rather than actualizing their potentiality as a human being” . “when it comes to observing faults in others, most of us are are quick to react with condemnation. But what about looking inwardly for a change? To personally examine who we really are and what we are, our merits as well as our faults — in short, to see oneself as [one] is for once and to take responsibility [for] oneself.” . “I am happy because I am daily growing and honestly not knowing where the limit will yet lie.” . “I don’t believe in the manipulation game of creating a self image robot.” . Bruce Lee . “whoever has learned how to listen to trees no longer wants to be a tree [but] wants to be nothing except what he is.” . Hermann Hesse . “mycelium of inspiration” . “When I am sad, I like to imagine myself becoming a tree. Branches that bend without breaking, fractal with possibility, reaching resolutely toward the light. Roots touching the web of belonging beneath the surface of the world, that majestic mycelial network succoring and nurturing and connecting tree to tree — connection so effortless, so imperturbable, so free from the fragility of human relationships.” . Maria Papova, Brain Pickings . What do I see in this picture? . The image is titled “Perspective” taken by Maria Papova .",
            "url": "https://varadan13.github.io/import_this/metaphor%20archive/2021/03/25/March-metaphors.html",
            "relUrl": "/metaphor%20archive/2021/03/25/March-metaphors.html",
            "date": " • Mar 25, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://varadan13.github.io/import_this/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "ME -&gt; .",
          "url": "https://varadan13.github.io/import_this/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://varadan13.github.io/import_this/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}